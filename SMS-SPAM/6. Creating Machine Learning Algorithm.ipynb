{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b73c7b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import pandas as pd\n",
    "import random \n",
    "import pickle\n",
    "\n",
    "BASE_DIR = pathlib.Path().resolve().parent #Check: /Users/kristoffervarslott/Documents/Python.py/AI_API\n",
    "SMS_SPAM_DIR = pathlib.Path().resolve()    #Check: /Users/kristoffervarslott/Documents/Python.py/AI_API/SMS-SPAM\n",
    "DATASETS_DIR = BASE_DIR / 'Datasets' #Check: /Users/kristoffervarslott/Documents/Python.py/AI_API/Datasets\n",
    "\n",
    "ZIPS_DIR = DATASETS_DIR / 'Zips'     #Check: /Users/kristoffervarslott/Documents/Python.py/AI_API/Datasets/Zips\n",
    "ZIPS_DIR.mkdir(exist_ok = True, parents = True)\n",
    "\n",
    "#Spam-Classifier folder: START\n",
    "SPAM_CLASSIFIER_DIR = DATASETS_DIR / 'Spam-Classifier'\n",
    "\n",
    "SMS_SPAM_DIR = SPAM_CLASSIFIER_DIR / 'Sms-Spam'\n",
    "SMS_SPAM_DIR.mkdir(exist_ok = True, parents = True)\n",
    "\n",
    "YOUTUBE_SPAM_DIR = SPAM_CLASSIFIER_DIR / 'Youtube-Spam'\n",
    "YOUTUBE_SPAM_DIR.mkdir(exist_ok = True, parents = True)\n",
    "#Spam-Classifier folder : END\n",
    "\n",
    "#Exports folder: START\n",
    "EXPORT_DIR = DATASETS_DIR / 'Exports'\n",
    "EXPORT_DIR.mkdir(exist_ok = True, parents = True)\n",
    "SPAM_DATASETS_DIR = EXPORT_DIR / 'Spam_Dataset.csv'\n",
    "METADATA_EXPORT_PATH = EXPORT_DIR / 'Spam-Metadata.json'\n",
    "METADATA_EXPORT_PATH_pkl = EXPORT_DIR / 'Spam-Metadata.pkl'\n",
    "TOKENIZER_EXPORT_PATH = EXPORT_DIR / 'Spam-Tokenizer.json'\n",
    "#Exports folder: END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ee6dd168",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "\n",
    "#Opening META_EXPORT_PATH datafile called Spam-Metadata.pkl\n",
    "#USing pickle to load dataset and storing data in dictonary called data.\n",
    "with open (METADATA_EXPORT_PATH_pkl, 'rb') as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3db65ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = data['X_train']\n",
    "X_test  = data['X_test']\n",
    "y_train = data['y_train']\n",
    "y_test  = data['y_test']\n",
    "max_words = data['max_words']\n",
    "max_seq_len = data['max_seq_len']\n",
    "label_legend = data['label_legend']\n",
    "label_legend_inverted = data['label_legend_inverted']\n",
    "tokenizer = data['tokenizer']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "51811319",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "69e78efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_json = {}\n",
    "\n",
    "#Opening TOKENIZER_EXPORT_PATH datafile called Spam-Tokenizer.json\n",
    "#USing json to load dataset and storing data in dictonary called data_json.\n",
    "with open (TOKENIZER_EXPORT_PATH, 'rb') as f:\n",
    "    data_json = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2df4052d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Embedding, LSTM, SpatialDropout1D\n",
    "from tensorflow.keras.models import Model, Sequential \n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6b4fe965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 500, 128)          128000    \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_5 (Spatial (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 196)               254800    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 2)                 394       \n",
      "=================================================================\n",
      "Total params: 383,194\n",
      "Trainable params: 383,194\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 128\n",
    "lstm_out = 196\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words, embed_dim, input_length = X_train.shape[1]))\n",
    "model.add(SpatialDropout1D(0.4))\n",
    "model.add(LSTM(lstm_out, dropout=0.3, recurrent_dropout=0.3))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3512bb59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      "158/158 [==============================] - 133s 824ms/step - loss: 0.2638 - accuracy: 0.8971 - val_loss: 0.1365 - val_accuracy: 0.9565\n",
      "Epoch 2/7\n",
      "158/158 [==============================] - 138s 874ms/step - loss: 0.1013 - accuracy: 0.9677 - val_loss: 0.1045 - val_accuracy: 0.9694\n",
      "Epoch 3/7\n",
      "158/158 [==============================] - 137s 865ms/step - loss: 0.0763 - accuracy: 0.9784 - val_loss: 0.1154 - val_accuracy: 0.9686\n",
      "Epoch 4/7\n",
      "158/158 [==============================] - 138s 874ms/step - loss: 0.0590 - accuracy: 0.9839 - val_loss: 0.1149 - val_accuracy: 0.9706\n",
      "Epoch 5/7\n",
      "158/158 [==============================] - 137s 867ms/step - loss: 0.0610 - accuracy: 0.9826 - val_loss: 0.1153 - val_accuracy: 0.9662\n",
      "Epoch 6/7\n",
      "158/158 [==============================] - 137s 869ms/step - loss: 0.0522 - accuracy: 0.9833 - val_loss: 0.1188 - val_accuracy: 0.9690\n",
      "Epoch 7/7\n",
      "158/158 [==============================] - 139s 877ms/step - loss: 0.0428 - accuracy: 0.9869 - val_loss: 0.1263 - val_accuracy: 0.9698\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x16aab53a0>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Starting actual training\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 7\n",
    " \n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), batch_size = batch_size, verbose = 1, epochs = epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8bfeb164",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_EXPORT_PATH = EXPORT_DIR / 'Spam_Model.h5'\n",
    "model.save(str(MODEL_EXPORT_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f472e851",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#Predicting data: \n",
    "\n",
    "def predict(text_str, max_words = 1000, max_sequences = 500, tokenizer = None):\n",
    "    if not tokenizer:\n",
    "        return None\n",
    "    sequences   = tokenizer.texts_to_sequences([text_str]) #Converting input text to sequences from tokenizer \n",
    "    x_input     = pad_sequences(sequences, maxlen = max_sequences) #Padding the x-input for formatting\n",
    "    y_output    = model.predict(x_input) #passing in x-input in correct format and sequence to model.predict()\n",
    "    \n",
    "    #top_y_input = np.argmax(y_output) #Collecting index of largest value example: ([0.9837, 0.0167]), yields index 0\n",
    "    preds = y_output[0]\n",
    "   \n",
    "    labeled_preds = [{f'{label_legend_inverted[str(i)]}': x} for i, x in enumerate(preds)]\n",
    "    return labeled_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "0cb1bfc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'ham': 0.001855096}, {'spam': 0.99814487}]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict('Get a huge discount on TV by calling this number 93718738, and visit this webpage https://stackoverflow.com/questions/19537520/attributeerror-nonetype-object-has-no-attribute-lower-python', max_words = max_words, max_sequences = max_seq_len, tokenizer = tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff8da28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6f3c54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98299fb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a637cc05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
